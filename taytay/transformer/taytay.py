# -*- coding: utf-8 -*-
"""taytay.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GJZPZU3PaB54uwgIrXKSllxvbrAmDtUb
"""

import torch
import time
import math
import random
import torch.nn as nn
#import matplotlib.pyplot as plt
#from IPython.display import clear_output


M = ['~', '$', ' ', '\n', '\'', ',']
for i in range(26):
    M.append(chr(ord('a') + i))
vocab_size = len(M)

def tokenize(str):
  tokens = []
  for char in str:
      index = M.index(char) if char in M else 1
      tokens.append(index)
  return tokens

def untokenize(ids):
  chars = []
  id_list = ids.tolist()
  for idx in id_list:
      char = M[idx]
      chars.append(char)
  return ''.join(chars)

def token_to_vec(token):
  out = torch.zeros(vocab_size)
  out[token] = 1
  return out

def tokens_to_vecs(tokensArrays):
  return torch.stack([torch.stack([token_to_vec(token) for token in tokens]) for tokens in tokensArrays])

class PositionalEncoding(nn.Module):

    def __init__(self, d_model, context_length):
        super().__init__()

        position = torch.arange(context_length).unsqueeze(1)
        div_term = torch.arange(0, d_model, 2) / 2 + 1
        pe = torch.zeros(context_length, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        Arguments:
            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``
        """
        x = x + self.pe[:x.size(0)]
        return x

class MaskedSelfAttention(nn.Module):

  def __init__(self, key_dim, embedding_dim, value_dim, dropout):
    super().__init__()
    self.key_dim = key_dim
    query_dim = key_dim
    self.K = nn.Linear(embedding_dim, key_dim, bias=False)
    self.Q = nn.Linear(embedding_dim, query_dim, bias=False)
    self.V = nn.Linear(embedding_dim, value_dim, bias=False)
    self.softmax = nn.Softmax(dim=2)
    self.dropout = nn.Dropout(dropout)


  def forward(self, x):
    """
    Arguments:
        x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``
    """
    key = self.K(x)
    query = self.Q(x)
    value = self.V(x)

    scores = torch.matmul(query, key.transpose(1, 2)) * self.key_dim ** -0.5

    scores = self.softmax(scores.masked_fill(torch.tril(scores) == 0, float('-inf')))
    x = torch.matmul(scores, value)
    return self.dropout(x)

class FFN(nn.Module):

    def __init__(self, emebedding_dim, dim, n_layers, dropout):
        super().__init__()
        self.n_hidden_layers = n_layers
        self.input_fc = nn.Linear(emebedding_dim, dim)
        self.hidden_fc = nn.ModuleList([nn.Linear(dim, dim) for i in range(n_layers)])
        self.relu = nn.ReLU()
        self.proj = nn.Linear(dim, emebedding_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        """
        Arguments:
            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``
        """
        x = self.input_fc(x)
        x = self.relu(x)
        for i in range(self.n_hidden_layers):
            x = self.hidden_fc[i](x)
            x = self.relu(x)
        x = self.proj(x)
        return self.dropout(x)

class MultiHeadAttention(nn.Module):

    def __init__(self, key_dim, embedding_dim, n_heads, dropout):
        super().__init__()
        value_dim = embedding_dim // n_heads
        self.msas = nn.ModuleList([MaskedSelfAttention(key_dim, embedding_dim, value_dim, dropout) for i in range(n_heads)])
        self.proj = nn.Linear(embedding_dim, embedding_dim)
        self.dropout = nn.Dropout(dropout)
    def forward(self, x):
        """
        Arguments:
            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``
        """
        x = torch.cat([self.msas[i](x) for i in range(len(self.msas))], dim=-1)
        x = self.proj(x)
        return self.dropout(x)

class DecoderBlock(nn.Module):

    def __init__(self, embedding_dim, key_dim, n_heads, hidden_dim, ffn_layers, dropout):
        super().__init__()
        self.msa = MultiHeadAttention(key_dim, embedding_dim, n_heads, dropout)
        self.ffn = FFN(embedding_dim, hidden_dim, ffn_layers, dropout)
        self.ln1 = nn.LayerNorm(embedding_dim)
        self.ln2 = nn.LayerNorm(embedding_dim)

    def forward(self, x):
        """
        Arguments:
            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``
        """
        x = x + self.msa(self.ln1(x))
        x = x + self.ffn(self.ln2(x))
        return x


class TayTay(nn.Module):

    def __init__(
        self,
        embedding_dim=2,
        key_dim=2,
        n_heads=1,
        scaling_factor=1,
        ffn_layers=0,
        decoder_layers=1,
        dropout=0.2,
        context_length=5000
      ):
        super().__init__()
        self.emb = nn.Linear(vocab_size, embedding_dim, bias=False)
        self.positional_encoding = PositionalEncoding(embedding_dim, context_length)

        hidden_dim = embedding_dim * scaling_factor
        self.decoder = nn.ModuleList([
            DecoderBlock(embedding_dim, key_dim, n_heads, embedding_dim, ffn_layers, dropout)
            for i in range(decoder_layers)
        ])
        self.ln = nn.LayerNorm(embedding_dim)

        self.lm_head = nn.Linear(embedding_dim, vocab_size)
        self.softmax = nn.Softmax(dim=2)

    def forward(self, x):
        """
        Arguments:
            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``
        """
        x = self.emb(x)
        x = self.positional_encoding(x)
        for i in range(len(self.decoder)):
          x = self.decoder[i](x)
        x = self.lm_head(self.ln(x))
        x = self.softmax(x)
        return x

    def gen(self, input_text):
      for i in range(10):
        tokens = tokenize(input_text)
        input_ids = tokens_to_vecs([tokens]).transpose(0, 1).to(device)
        preds = torch.argmax(self.forward(input_ids), dim=2).transpose(0, 1)
        new_token = untokenize(torch.tensor([preds[0][-1]]))
        input_text = input_text + '' + new_token
      return input_text

# https://www.youtube.com/watch?v=kCc8FmEb1nY
class TayTayDatasetNaive(torch.utils.data.Dataset):
  def __init__(self, filename, chunk_size):
    with open(filename, 'r') as f:
      self.text = f.read()
      total_length = len(self.text)

      self.target_length = math.ceil(1.2 * total_length / chunk_size)
      target_length = self.target_length

      pieces = []
      for i in range(target_length):
        offset = random.randint(0, total_length - chunk_size - 1)
        x = tokens_to_vecs([tokenize(self.text[offset:offset + chunk_size])])
        y = tokens_to_vecs([tokenize(self.text[offset + 1:offset + chunk_size + 1])])
        pieces.append((x, y))

      self.tokensArray = pieces

  def __len__(self):
    return self.target_length

  def __getitem__(self, idx):
    return self.tokensArray[idx]

def train(model, train_loader, val_loader, optimizer, loss, epochs, checkpoint_name):
  min_v_loss = float('inf')
  all_v = []
  all_t = []

  for epoch in range(epochs):
    start = time.time()
    model.train()
    total_loss = 0
    for (x, y) in train_loader:
      x = x.to(device)
      y = y.to(device)
      output = model(x)
      l = loss(output, y)
      l.backward()
      optimizer.step()
      optimizer.zero_grad()
      total_loss += l

    val_loss = 0
    model.eval()
    for (x, y) in val_loader:
      x = x.to(device)
      y = y.to(device)
      output = model(x)
      l = loss(output, y)
      val_loss += l

    avg_t_loss = total_loss.detach().cpu().numpy()
    avg_v_loss = val_loss.detach().cpu().numpy() * 9
    all_v.append(avg_v_loss)
    all_t.append(avg_t_loss)

    if avg_v_loss < min_v_loss:
      min_v_loss = avg_v_loss
      torch.save(model.state_dict(), checkpoint_name + '.pt')

    end = time.time()
    deltaT = end - start
    scheduler.step()
    ETA = deltaT * (epochs - epoch - 1)
    #clear_output()
    #plt.plot([i for i in range(len(all_t))], all_t, label='train')
    #plt.plot([i for i in range(len(all_v))], all_v, label='val')
    #plt.legend()
    #plt.show()
    print(
        '\ntrain loss: {}, val loss: {}, took {:.2f}s, ETA: {:.2f}s,\ntest generation: {}'
          .format(avg_t_loss, avg_v_loss, end - start, ETA, model.gen('i love y'))
    )
  return all_t, all_v

print('Loading dataset...')
dataset = TayTayDatasetNaive('songs.txt', chunk_size=64)
firstx, firsty = dataset[0]
print(
    'Data length: ',
    len(dataset),
    'Example: ',
    untokenize(torch.argmax(firstx.transpose(0, 1)[0], dim=1))
)

def collate_fn(batch):
  x = torch.stack([x[0][0] for x in batch])
  y = torch.stack([y[1][0] for y in batch])
  return (x, y)

train_split, val_split = torch.utils.data.random_split(dataset, [0.9, 0.1])
train_loader = torch.utils.data.DataLoader(train_split, batch_size=64, collate_fn=collate_fn, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_split, batch_size=64, collate_fn=collate_fn, shuffle=True)
print('Training set has {} instances'.format(len(train_split)))
print('Validation set has {} instances'.format(len(val_split)))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print('Using ', device)

#model = torch.load('model.pt').to(device)
model = TayTay(embedding_dim=72, n_heads=3, scaling_factor=1.5, ffn_layers=2, decoder_layers=2, dropout=0.05).to(device)

# see all parameters
n_params = 0
for name, param in model.named_parameters():
    a = 1
    for i in param.shape:
        a *= i
    n_params += a
    #print(name, param.shape, a)
    #print(param.tolist())

print('Parameters: ', n_params)

loss = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)

all_t, all_v = train(model, train_loader, val_loader, optimizer, loss, 10000, 'taytay-autosave')

